import torch
import time
import torch.profiler
from modeling_llama_HLA import LlamaForCausalLM ,LlamaConfig # ç¡®ä¿ä½ æœ‰ modeling_llama.py æ–‡ä»¶
model_name='AICrossSim/clm-60m'
# é€‰æ‹©è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½ LLaMA é…ç½®
config = LlamaConfig.from_pretrained(model_name)  # âœ… è¿™è¡Œä¸æ˜¯å¿…é¡»çš„

# âœ… æ­£ç¡®åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = LlamaForCausalLM.from_pretrained(model_name).to(device)

model.train()  # å¯ç”¨è®­ç»ƒæ¨¡å¼ï¼ˆDropout / åå‘ä¼ æ’­ï¼‰

# ç”Ÿæˆæµ‹è¯•è¾“å…¥
batch_size = 1
seq_length = 128  # ä¾‹å¦‚ï¼š128 ä¸ª token
input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length)).to(device)
attention_mask = torch.ones(batch_size, seq_length).to(device)
labels = torch.randint(0, config.vocab_size, (batch_size, seq_length)).to(device)  # ç”Ÿæˆéšæœºæ ‡ç­¾

# **æµ‹é‡å„ä¸ªç¯èŠ‚è€—æ—¶**
with torch.no_grad():  # å…ˆæµ‹ Forward
    torch.cuda.synchronize()
    start_time = time.time()

    # **1. è¯åµŒå…¥ï¼ˆEmbeddingï¼‰**
    embed_start = time.time()
    inputs_embeds = model.model.embed_tokens(input_ids)
    torch.cuda.synchronize()
    embed_time = time.time() - embed_start

    # **2. æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**
    rope_start = time.time()
    position_ids = torch.arange(seq_length, device=device).unsqueeze(0)
    position_embeddings = model.model.rotary_emb(inputs_embeds, position_ids)
    torch.cuda.synchronize()
    rope_time = time.time() - rope_start

    # **3. Transformer å±‚å‰å‘ä¼ æ’­**
    transformer_start = time.time()
    hidden_states = inputs_embeds
    layer_times = []

    for layer in model.model.layers:
        layer_start = time.time()
        hidden_states = layer(
            hidden_states,
            attention_mask=attention_mask,
            position_embeddings=position_embeddings
        )[0]
        torch.cuda.synchronize()
        layer_time = time.time() - layer_start
        layer_times.append(layer_time)

    torch.cuda.synchronize()
    transformer_time = time.time() - transformer_start

    # **4. RMSNormï¼ˆå½’ä¸€åŒ–ï¼‰**
    norm_start = time.time()
    hidden_states = model.model.norm(hidden_states)
    torch.cuda.synchronize()
    norm_time = time.time() - norm_start

    # **5. è¾“å‡ºæŠ•å½±ï¼ˆLM Headï¼‰**
    lm_head_start = time.time()
    logits = model.lm_head(hidden_states)
    torch.cuda.synchronize()
    lm_head_time = time.time() - lm_head_start

    # **æ€»å‰å‘ä¼ æ’­æ—¶é—´**
    total_forward_time = time.time() - start_time

# **æµ‹é‡åå‘ä¼ æ’­**
loss_fn = torch.nn.CrossEntropyLoss()
logits = logits.view(-1, config.vocab_size)  # å˜å½¢ä»¥é€‚é… loss
labels = labels.view(-1)  # å˜å½¢ä¸º 1D
loss = loss_fn(logits, labels)

# **6. è®¡ç®—æ¢¯åº¦ï¼ˆBackward Passï¼‰**
torch.cuda.synchronize()
backward_start = time.time()
loss.backward()
torch.cuda.synchronize()
backward_time = time.time() - backward_start

# **æ‰“å°è€—æ—¶**
print("\nğŸ“Š **å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰å„å­ç¯èŠ‚è€—æ—¶åˆ†æ** ğŸ“Š")
print(f"ğŸ•’ è¯åµŒå…¥ï¼ˆEmbeddingï¼‰ï¼š{embed_time:.4f} s")
print(f"ğŸ•’ æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼š{rope_time:.4f} s")
for i, t in enumerate(layer_times):
    print(f"ğŸ•’ Transformer å±‚ {i}ï¼ˆAttention + MLPï¼‰ï¼š{t:.4f} s")
print(f"ğŸ•’ Transformer å±‚æ€»è€—æ—¶ï¼š{transformer_time:.4f} s")
print(f"ğŸ•’ å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰ï¼š{norm_time:.4f} s")
print(f"ğŸ•’ è¾“å‡ºæŠ•å½±ï¼ˆLM Headï¼‰ï¼š{lm_head_time:.4f} s")
print(f"ğŸ•’ **æ€»å‰å‘ä¼ æ’­æ—¶é—´**ï¼š{total_forward_time:.4f} s")

print("\nğŸ“Š **åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰è€—æ—¶åˆ†æ** ğŸ“Š")
print(f"ğŸ•’ åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰ï¼š{backward_time:.4f} s")
print(f"ğŸ•’ **å®Œæ•´è®­ç»ƒæ­¥ï¼ˆForward + Backwardï¼‰æ€»æ—¶é—´**ï¼š{total_forward_time + backward_time:.4f} s")

# **ä½¿ç”¨ PyTorch Profiler è¿›è¡Œè¯¦ç»†åˆ†æ**
print("\nğŸ“Š è¯¦ç»†æ€§èƒ½åˆ†æï¼ˆtorch.profilerï¼‰")
with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    loss = model(input_ids, attention_mask=attention_mask, labels=labels).loss
    loss.backward()

# æ‰“å°æ€§èƒ½åˆ†æç»“æœ
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
