import torch
from transformers import AutoTokenizer
from modeling_llama import LlamaForCausalLM, LlamaConfig


def check_tokenizer_model_consistency(model_dir, tokenizer_path=None, test_prompt="The book is about"):
    # åŠ è½½ tokenizer
    tokenizer_path = tokenizer_path or model_dir
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    # æ£€æŸ¥ç‰¹æ®Š token
    print("\nğŸ” Special Tokens æ£€æŸ¥ï¼š")
    print("PAD token:", tokenizer.pad_token)
    print("UNK token:", tokenizer.unk_token)
    print("EOS token:", tokenizer.eos_token)
    print("BOS token:", tokenizer.bos_token)

    # æ£€æŸ¥ vocab size
    print("\nğŸ”¢ Vocab & Embedding å°ºå¯¸æ£€æŸ¥ï¼š")
    print("Tokenizer vocab size:", len(tokenizer))

    config = LlamaConfig.from_pretrained(model_dir)
    model = LlamaForCausalLM.from_pretrained(model_dir, config=config).eval()

    embedding_size = model.get_input_embeddings().weight.shape[0]
    print("Model embedding size:", embedding_size)

    if len(tokenizer) != embedding_size:
        print("âŒ â— vocab size â‰  embedding sizeï¼Œæ¨¡å‹å’Œ tokenizer ä¸åŒ¹é…ï¼")
    else:
        print("âœ… âœ”ï¸ vocab size ä¸ embedding size åŒ¹é…")

    # åˆ†è¯æµ‹è¯•
    print("\nğŸ§ª åˆ†è¯æµ‹è¯•ï¼š")
    tokens = tokenizer.tokenize(test_prompt)
    ids = tokenizer.convert_tokens_to_ids(tokens)
    decoded = tokenizer.decode(tokenizer.encode(test_prompt))

    print("åŸå§‹æ–‡æœ¬:", test_prompt)
    print("Tokenizer tokens:", tokens)
    print("Token IDs:", ids)
    print("Decoded back:", decoded)

    # æ¨¡å‹ç”Ÿæˆæµ‹è¯•
    print("\nğŸš€ æ¨¡å‹ç”Ÿæˆæ£€æŸ¥ï¼š")
    inputs = tokenizer(test_prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(model.device)
    with torch.no_grad():
        output = model.generate(input_ids=input_ids, max_new_tokens=30)

    gen_ids = output[0].tolist()
    gen_text = tokenizer.decode(output[0], skip_special_tokens=True)

    print("ğŸ§  ç”Ÿæˆ Token IDs:", gen_ids)
    print("ğŸ“ ç”Ÿæˆæ–‡æœ¬:", gen_text)

    # UNK æ£€æŸ¥
    unk_count = sum(1 for t in tokenizer.convert_ids_to_tokens(gen_ids) if t == tokenizer.unk_token)
    if unk_count > 0:
        print(f"\nâš ï¸ æ£€æµ‹åˆ°ç”Ÿæˆä¸­åŒ…å« {unk_count} ä¸ª <unk> tokenï¼Œå¯èƒ½æ˜¯è¯è¡¨å¤ªå°æˆ–ä¸åŒ¹é…")
    else:
        print("\nâœ… æœªæ£€æµ‹åˆ° <unk> tokenï¼Œç”Ÿæˆæ­£å¸¸")


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    model_dir = r"E:\MHA2HLA\zzq_evaluation\best_epoch_7"
    tokenizer_path = "HuggingFaceTB/cosmo2-tokenizer"  # æˆ–ç›´æ¥ç”¨ model_dir
    check_tokenizer_model_consistency(model_dir, tokenizer_path)
