import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset, DatasetDict, load_from_disk
from collections import defaultdict
from modeling_llama import LlamaForCausalLM ,LlamaConfig # âœ… ä»è‡ªå®šä¹‰ LLaMA ç»“æ„å¯¼å…¥
import torch
from transformers import EarlyStoppingCallback
torch.cuda.empty_cache()
torch.cuda.ipc_collect()

# âœ… 1. **è§£æå‘½ä»¤è¡Œå‚æ•°**
parser = argparse.ArgumentParser(description="Load and fine-tune LLaMA model.")
parser.add_argument("--model_dir", type=str, required=True, help="Path to the model directory.")
parser.add_argument("--model_name", type=str, default="AICrossSim/clm-60m", help="Hugging Face model name.")
parser.add_argument("--batch_size", type=int, default=4, help="Batch size for training and evaluation.")
parser.add_argument("--epochs", type=int, default=5, help="Number of training epochs.")
args = parser.parse_args()

device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… 2. **å®šä¹‰è·¯å¾„**
model_dir = args.model_dir
model_name = args.model_name
dataset_path = os.path.join(model_dir, "wikitext-2-v1")
model_path = os.path.join(model_dir, "model.safetensors")
tokenizer_path = os.path.join(model_dir, "tokenizer.json")

# âœ… 3. **ç¡®ä¿æ¨¡å‹æƒé‡å­˜åœ¨**
print(f"ğŸ”„ ä¸‹è½½æ¨¡å‹å’Œ tokenizerï¼š{model_name}")

config = LlamaConfig.from_pretrained(model_name)
model = LlamaForCausalLM.from_pretrained(model_name, config=config).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name)
# âœ… 5. **ç¡®ä¿ Tokenizer æœ‰ `pad_token`**
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

os.makedirs(model_dir, exist_ok=True)
model.save_pretrained(model_dir, safe_serialization=True)
tokenizer.save_pretrained(model_dir)


# âœ… 6. **æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨**
expected_splits = ["train", "validation", "test"]
if all(os.path.exists(os.path.join(dataset_path, split)) for split in expected_splits):
    print(f"ğŸ“‚ å‘ç°æœ¬åœ°æ•°æ®é›† {dataset_path}ï¼Œæ­£åœ¨åŠ è½½...")
    dataset = DatasetDict({
        "train": load_from_disk(os.path.join(dataset_path, "train")),
        "validation": load_from_disk(os.path.join(dataset_path, "validation")),
        "test": load_from_disk(os.path.join(dataset_path, "test"))
    })
else:
    print("ğŸ” æ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä» Hugging Face ä¸‹è½½ `wikitext-2-v1`...")
    dataset = load_dataset("wikitext", "wikitext-2-v1")
    dataset.save_to_disk(dataset_path)
    print(f"âœ… æ•°æ®é›†å·²ä¸‹è½½å¹¶ä¿å­˜è‡³ {dataset_path}")

print(f"ğŸ“‚ æ•°æ®é›†åŠ è½½æˆåŠŸï¼š{dataset}")

# âœ… 7. **Tokenization**
def tokenize_function(examples):
    tokens = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# âœ… 8. **è®­ç»ƒå‚æ•°**
training_args = TrainingArguments(
    output_dir=model_dir,
    per_device_train_batch_size=args.batch_size,
    per_device_eval_batch_size=args.batch_size,
    num_train_epochs=args.epochs,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir=os.path.join(model_dir, "logs"),
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    fp16=torch.cuda.is_available(),
    push_to_hub=False,
    report_to="none",
    logging_first_step=True,
    load_best_model_at_end=True,            # ğŸ‘‰ åŠ è½½éªŒè¯é›†æœ€ä¼˜æ¨¡å‹
    metric_for_best_model="eval_loss",      # ğŸ‘‰ ä»¥ eval_loss ä¸ºåˆ¤æ–­æ ‡å‡†
    greater_is_better=False,                # ğŸ‘‰ eval_loss è¶Šä½è¶Šå¥½
    save_total_limit=3,                     # ğŸ‘‰ åªä¿ç•™ä¸€ä¸ªæœ€ä½³æ¨¡å‹
)

# âœ… 9. **è®­ç»ƒæ¨¡å‹**
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

train_result = trainer.train()

log_history = trainer.state.log_history

# è®¡ç®—æ¯ä¸ª epoch çš„å¹³å‡ Train Loss
epoch_loss = defaultdict(list)
epoch_eval_loss = {}

for entry in log_history:
    if "loss" in entry and "epoch" in entry:
        epoch_loss[entry["epoch"]].append(entry["loss"])
    if "eval_loss" in entry and "epoch" in entry:
        epoch_eval_loss[entry["epoch"]] = entry["eval_loss"]  # åªå–æœ€åä¸€æ¬¡è¯„ä¼° loss

# è®¡ç®—æ¯ä¸ª epoch çš„å¹³å‡ loss
epochs = sorted(epoch_loss.keys())
train_loss_per_epoch = [sum(losses) / len(losses) for losses in epoch_loss.values()]

# è·å–éªŒè¯é›† loss
eval_epochs = sorted(epoch_eval_loss.keys())
eval_loss_per_epoch = [epoch_eval_loss[epoch] for epoch in eval_epochs]

# ç”»å›¾
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_loss_per_epoch, label="Train Loss", linestyle="--", marker="o")
plt.plot(eval_epochs, eval_loss_per_epoch, label="Validation Loss", linestyle="-", marker="s")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Validation Loss Curve")
plt.legend()
plt.grid()
plt.savefig(os.path.join(model_dir, "loss_curve_epoch.png"))
plt.show()

# âœ… 11. **ä¿å­˜æ¨¡å‹**
model.save_pretrained(model_dir)
tokenizer.save_pretrained(model_dir)
print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_dir}")

# âœ… 12. **æµ‹è¯•é›†æ¨ç†**
print("ğŸ” å¼€å§‹åœ¨ `test` æ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†...")
test_results = trainer.predict(tokenized_datasets["test"])

# è§£æ `test` ç»“æœ
test_preds = torch.argmax(torch.tensor(test_results.predictions), dim=-1)
test_texts = [tokenizer.decode(pred, skip_special_tokens=True) for pred in test_preds]

# âœ… 13. **ä¿å­˜ `test` è¾“å…¥å’Œè¾“å‡ºåˆ° Excel**
test_df = pd.DataFrame({
    "input_text": dataset["test"]["text"],
    "predicted_text": test_texts
})

excel_path = os.path.join(model_dir, "test_results.xlsx")
test_df.to_excel(excel_path, index=False, encoding="utf-8")
print(f"âœ… æµ‹è¯•é›†ç»“æœå·²ä¿å­˜è‡³ {excel_path}")

# âœ… 14. **å°è¯•è¯„ä¼°**
try:
    from lm_eval.evaluator import simple_evaluate
    results = simple_evaluate(
        model=model,
        tasks=["wikitext"],
        batch_size=args.batch_size,
    )
    print("ğŸ“Š Evaluation Results:", results)
except Exception as e:
    print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")

print("ğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼ğŸš€")

