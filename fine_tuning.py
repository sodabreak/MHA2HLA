import os
import torch
from transformers import LlamaForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import google.protobuf
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Device Count: {torch.cuda.device_count()}")
print(f"Current CUDA Device: {torch.cuda.current_device()}" if torch.cuda.is_available() else "No GPU detected")
print(f"GPU Name: {torch.cuda.get_device_name(0)}" if torch.cuda.is_available() else "No GPU detected")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Using device: {device}")

# âœ… 0. **é¿å… protobuf ä¾èµ–é—®é¢˜**
#os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

# âœ… 1. **ç¡®ä¿æ¨¡å‹æƒé‡å’Œ tokenizer å­˜åœ¨**
model_dir = "E:/MHA2HLA/clm-60m-finetuned"
model_name = "AICrossSim/clm-60m"

if not os.path.exists(model_dir):
    print("ğŸ” Model not found locally. Downloading from Hugging Face...")
    model = LlamaForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    os.makedirs(model_dir, exist_ok=True)
    model.save_pretrained(model_dir)
    tokenizer.save_pretrained(model_dir)
else:
    print("âœ… Model found locally. Loading...")
    model = LlamaForCausalLM.from_pretrained(model_dir)
    tokenizer = AutoTokenizer.from_pretrained(model_dir)

# **ç¡®ä¿ Tokenizer æœ‰ `pad_token`**
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… 2. **åŠ è½½ Parquet æ•°æ®**
dataset_path = "E:/MHA2HLA/clm-60m-finetuned/wikitext-2-v1"

dataset = load_dataset("parquet", data_files={
    "train": f"{dataset_path}/train-00000-of-00001.parquet",
    "validation": f"{dataset_path}/validation-00000-of-00001.parquet",
    "test": f"{dataset_path}/test-00000-of-00001.parquet",
})

print("ğŸ“‚ æ•°æ®é›†ä¿¡æ¯:", dataset)

# **æŸ¥çœ‹æ•°æ®ç»“æ„**
print("ğŸ” æ ·æœ¬æ•°æ®:", dataset["train"][0])

# âœ… 3. **Tokenization**
def tokenize_function(examples):
    tokens = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    tokens["labels"] = tokens["input_ids"].copy()  # æ·»åŠ  `labels`
    return tokens


tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# âœ… 4. **è®­ç»ƒå‚æ•°**
training_args = TrainingArguments(
    output_dir=model_dir,
    per_device_train_batch_size=4,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="E:/MHA2HLA/logs",
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",
    fp16=torch.cuda.is_available(),
    push_to_hub=False,
    report_to="none",
)

# âœ… 5. **è®­ç»ƒæ¨¡å‹**
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
)

trainer.train()

# âœ… 6. **ä¿å­˜æ¨¡å‹**
model.save_pretrained(model_dir)
tokenizer.save_pretrained(model_dir)
print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³:", model_dir)
'''
# âœ… 7. **è¯„ä¼°**
try:
    from lm_eval.evaluator import simple_evaluate

    results = simple_evaluate(
        model=model,
        tasks=["wikitext"],
        batch_size=4,
    )
    print("ğŸ“Š Evaluation Results:", results)
except Exception as e:
    print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
'''
# âœ… 8. **ä¸Šä¼  Hugging Faceï¼ˆå¯é€‰ï¼‰**
upload_to_hf = False  # å¦‚æœéœ€è¦ä¸Šä¼ ï¼Œæ”¹æˆ True
if upload_to_hf:
    os.system(f"huggingface-cli upload {model_dir} --repo AICrossSim/clm-60m-finetuned")

print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ğŸš€")
