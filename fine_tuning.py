import os
import torch
import argparse
from transformers import LlamaForCausalLM, AutoTokenizer
from datasets import load_dataset, DatasetDict , load_from_disk
from transformers import TrainingArguments, Trainer

# âœ… 1. **è§£æå‘½ä»¤è¡Œå‚æ•°**
parser = argparse.ArgumentParser(description="Load and fine-tune LLaMA model.")
parser.add_argument("--model_dir", type=str, required=True, help="Path to the model directory.")
parser.add_argument("--model_name", type=str, default="AICrossSim/clm-60m", help="Hugging Face model name.")
args = parser.parse_args()

device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… 2. **å®šä¹‰è·¯å¾„**
model_dir = args.model_dir
model_name = args.model_name
dataset_path = os.path.join(model_dir, "wikitext-2-v1")  # âœ… æ•°æ®é›†ç›®å½•
model_path = os.path.join(model_dir, "model.safetensors")
tokenizer_path = os.path.join(model_dir, "tokenizer.json")

# âœ… 3. **ç¡®ä¿æ¨¡å‹æƒé‡å­˜åœ¨**
if not os.path.exists(model_path):
    print(f"ğŸ” Model not found locally in {model_dir}. Downloading from Hugging Face...")
    model = LlamaForCausalLM.from_pretrained(model_name).to(device)
    os.makedirs(model_dir, exist_ok=True)
    model.save_pretrained(model_dir, safe_serialization=True)
else:
    print(f"âœ… Model found locally in {model_dir}. Loading...")
    model = LlamaForCausalLM.from_pretrained(model_dir).to(device)

# âœ… 4. **ç¡®ä¿ Tokenizer å­˜åœ¨**
if not os.path.exists(tokenizer_path):
    print(f"ğŸ” Tokenizer not found locally in {model_dir}. Downloading from Hugging Face...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(model_dir)
else:
    print(f"âœ… Tokenizer found locally in {model_dir}. Loading...")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)

# âœ… 5. **ç¡®ä¿ Tokenizer æœ‰ `pad_token`**
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… 6. **æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨**
expected_splits = ["train", "validation", "test"]
if all(os.path.exists(os.path.join(dataset_path, split)) for split in expected_splits):
    print(f"ğŸ“‚ å‘ç°æœ¬åœ°æ•°æ®é›† {dataset_path}ï¼Œæ­£åœ¨åŠ è½½...")
    dataset = DatasetDict({
        "train": load_from_disk(os.path.join(dataset_path, "train")),
        "validation": load_from_disk(os.path.join(dataset_path, "validation")),
        "test": load_from_disk(os.path.join(dataset_path, "test"))
    })
else:
    print("ğŸ” æ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä» Hugging Face ä¸‹è½½ `wikitext-2-v1`...")
    dataset = load_dataset("wikitext", "wikitext-2-v1")

    # âœ… **å°†æ•°æ®é›†ä¿å­˜åˆ°æœ¬åœ°**
    dataset.save_to_disk(dataset_path)
    print(f"âœ… æ•°æ®é›†å·²ä¸‹è½½å¹¶ä¿å­˜è‡³ {dataset_path}")

print(f"ğŸ“‚ æ•°æ®é›†åŠ è½½æˆåŠŸï¼š{dataset}")

print("ğŸš€ Model, Tokenizer, and Dataset are ready!")


# **æŸ¥çœ‹æ•°æ®ç»“æ„**
print("æ ·æœ¬æ•°æ®:", dataset["train"][:5])  # æ‰“å°å‰ 5 æ¡

# âœ… 3. **Tokenization**
def tokenize_function(examples):
    tokens = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)
    tokens["labels"] = tokens["input_ids"].copy()  # æ·»åŠ  `labels`
    return tokens


tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# âœ… 4. **è®­ç»ƒå‚æ•°**
training_args = TrainingArguments(
    output_dir=model_dir,
    per_device_train_batch_size=4,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="E:/MHA2HLA/logs",
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",
    fp16=torch.cuda.is_available(),
    push_to_hub=False,
    report_to="none",
)

# âœ… 5. **è®­ç»ƒæ¨¡å‹**
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
)

trainer.train()

# âœ… 6. **ä¿å­˜æ¨¡å‹**
model.save_pretrained(model_dir)
tokenizer.save_pretrained(model_dir)
print("âœ… æ¨¡å‹å·²ä¿å­˜è‡³:", model_dir)


try:
    from lm_eval.evaluator import simple_evaluate

    results = simple_evaluate(
        model=model,
        tasks=["wikitext"],
        batch_size=4,
    )
    print("ğŸ“Š Evaluation Results:", results)
except Exception as e:
    print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")


print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ğŸš€")